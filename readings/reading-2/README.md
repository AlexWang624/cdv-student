## Reading 2
### **Response**
<br/>

The materials for Reading 2 are on the fairness and bias of algorithms. Algorithms in welfare systems pick the most deserving people and offer help to them. However, the algorithms are built with objectivity that assumes there is not enough for everyone, and there is triage within the algorithms. They neglect the “less-deserving” people and these people are not guaranteed access to the resources they need. This is defined as “empathy overwrite” in the interview with Virginia Eubanks. In addition, “technique glitches” could be used as excuses for any negative effects to cover up discriminations in these algorithms.

There are more specific examples in the materials we watch and listen to. For instance, people might find it hard to receive the resources they need using the algorithms. The example Eubanks mentions is that in order to enter the algorithm that will increase their chance to get an accommodation, vulnerable unhoused people in San Francisco need to fill out surveys that might lead them to incriminate themselves. People are forced into a dilemma of either admitting violating the law or giving up on a benefit they might receive. In this case, vulnerable people, who might not be benefited by the algorithms, become more vulnerable instead. The difficulty of accessing the resources made me think of the health code in post-pandemic China. The use of health code is based on the assumption that everyone has a smartphone and uses WeChat or Alipay. There was news about people of the older generations or from remote areas who do not have smartphones being rejected to take public transportation. The vulnerable people are deprived of their rights and are discriminated against. Another type of problem occurs during the operation of algorithms. The TED talk How I'm fighting bias in algorithms mentions a problem that occurs when the data collected for the algorithm are not diverse enough. I thought about the camera that catches people who are jaywalking in Shanghai. Sometimes it might not function properly and catches the images of people in the advertisements on the bus. Nowadays the government is using algorithms in fighting against crimes. Criminals can be targeted and located without sending policemen to find them. However, the negative effect for targeting the wrong person and mistaking him/her as the criminal is huge.
